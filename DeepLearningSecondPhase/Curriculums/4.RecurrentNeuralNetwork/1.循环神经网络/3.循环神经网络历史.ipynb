{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 历史一瞥\n",
    "\n",
    "循环神经网络演变历史如何？过去和现在分别如何发展呢？\n",
    "\n",
    "TDNN Time = Delay Neural Network 时延神经网络\n",
    "\n",
    "简单的RNN可以称之为Elman网络和Jordan网络\n",
    "\n",
    "LSTM = Long Short Term Memory 长短期记忆单元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"900\" height=\"580\" src=\"https://www.youtube.com/embed/HbxAnYUfRnc\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"900\" height=\"580\" src=\"https://www.youtube.com/embed/HbxAnYUfRnc\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如这个视频所提到的，循环神经网络的主要缺陷在于，采集超过8到10个步长的关系实际上难以实现。这个缺陷的原因在于\"梯度消失\"问题，其中信息贡献在一段时间内呈几何级消失。\n",
    "\n",
    "这意味着什么呢？\n",
    "\n",
    "你也许还记得，我们使用反向传播算法训练网络时的做法。在反向传播算法过程中，我们使用梯度调整权重矩阵。在这个过程中，通过导数的连续乘法来计算梯度。如果这些导数值可能太小，这些连续乘法可能实际上会导致梯度\"消失\"。\n",
    "\n",
    "`长短期记忆网络 (LSTM)` 可以解决循环神经网络中的梯度消失问题。\n",
    "\n",
    "如果你想了解更多[梯度消失](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)问题，或想进一步了解[等比数列](https://socratic.org/algebra/exponents-and-exponential-functions/geometric-sequences-and-exponential-functions)概念及其数值出现几何级下降，请使用这些资源。\n",
    "\n",
    "如果你仍感到好奇，想要了解这里提到重要里程碑的更多信息，请查看以下链接：\n",
    "\n",
    "- [时延神经网络](https://en.wikipedia.org/wiki/Time_delay_neural_network)\n",
    "\n",
    "- 这是1990年[Elman网络](http://onlinelibrary.wiley.com/doi/10.1207/s15516709cog1402_1/abstract)的论文原稿。此处提供链接，因为这是全球循环神经网络的重要里程碑。为了简便，你可以查阅下列[额外信息](https://en.wikipedia.org/wiki/Recurrent_neural_network#Elman_networks_and_Jordan_networks)。\n",
    "\n",
    "    - 在这个[LSTM](http://www.bioinf.jku.at/publications/older/2604.pdf)链接中，你可以找到论文原稿，作者是[Sepp Hochreiter](https://en.wikipedia.org/wiki/Sepp_Hochreiter)和[Jürgen Schmidhuber](http://people.idsia.ch/~juergen/)。不过你不需要了解全部细节。我们随后将介绍所有内容！\n",
    "    \n",
    "    \n",
    "    正如视频中提到的，长短期记忆网络（LSTMs）和门控循环单元（GRUs）通过帮助我们应用具有时间依赖的网络，提供了梯度消失问题的解决方案。这节课中我们将重点讨论循环神经网络，继续学习长短期记忆网络。我们不会重点关注门控循环单元。 如果想要了解门控循环单元的更多内容，可以参阅一下[博客](https://deeplearning4j.org/lstm.html)。重点关注标题为门控循环单元的综述。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
