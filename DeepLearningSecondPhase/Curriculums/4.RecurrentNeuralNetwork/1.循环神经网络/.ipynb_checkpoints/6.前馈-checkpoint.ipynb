{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前馈\n",
    "\n",
    "本节中，我们将仔细研究前馈过程背后的数学知识。通过使用基本的线性代数手段，这些计算非常简单！\n",
    "\n",
    "如果你对线性组合和矩阵乘法不很自信，可以使用以下链接复习： - [线性组合](http://linear.ups.edu/html/section-LC.html) - [矩阵乘法](https://en.wikipedia.org/wiki/Matrix_multiplication)\n",
    "\n",
    "假设只有一个隐藏层，我们在计算中会需要两个步骤。第一个是计算隐藏状态的数值，第二个是计算输出值。\n",
    "\n",
    "<img src=\"../../../sources/img/RNN/feedforward-6-1.png\">\n",
    "\n",
    "请注意，隐藏层和输出层都显示为向量，因为它们都是多个单一神经元表示的。\n",
    "\n",
    "第一个视频将帮你了解第一个步骤：计算隐藏状态的数值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"900\" height=\"550\" src=\"https://www.youtube.com/embed/4rCfnWbx8-0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"900\" height=\"550\" src=\"https://www.youtube.com/embed/4rCfnWbx8-0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"../../../sources/img/RNN/feedforward-6-2.png\">\n",
    "\n",
    "关于激活函数的更多信息以及使用方法可参考 [此处](https://github.com/Kulbear/deep-learning-nano-foundation/wiki/ReLU-and-Softmax-Activation-Functions)\n",
    "\n",
    "下一个视频可以帮你了解第二个步骤：计算输出值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"900\" height=\"550\" src=\"https://www.youtube.com/embed/kTYbTVh1d0k\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"900\" height=\"550\" src=\"https://www.youtube.com/embed/kTYbTVh1d0k\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如你在上面的视频中所见，计算输出向量的过程在数学上类似于计算隐藏层向量的过程。我们再次使用一个矩阵乘法的向量，然后使用一个激活函数。通过隐藏层和矩阵计算得出的最新向量，将隐藏层连接到输出起来。\n",
    "\n",
    "<img src=\"../../../sources/img/RNN/feedforward-6-3.png\">\n",
    "<img src=\"../../../sources/img/RNN/feedforward-6-4.png\">\n",
    "\n",
    "最常用的两个误差函数是[平方误差(MSE)](https://en.wikipedia.org/wiki/Mean_squared_error)(通常用于回归问题)和[交叉熵](https://www.ics.uci.edu/~pjsadows/notes.pdf)(通常用于分类问题)。\n",
    "\n",
    "在上述计算中，我们使用了平方误差的变体。\n",
    "\n",
    "接下来的几个视频将关注反向传播过程，即使用链式法则的随机梯度下降。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
