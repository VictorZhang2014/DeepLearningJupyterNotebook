{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"800\" height=\"450\" src=\"https://www.youtube.com/embed/MZL97-2joxQ\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"800\" height=\"450\" src=\"https://www.youtube.com/embed/MZL97-2joxQ\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 反向传播\n",
    "\n",
    "如何让多层神经网络学习呢？我们已了解了使用梯度下降来更新权重，反向传播算法则是它的一个延伸。以一个两层神经网络为例，可以使用链式法则计算输入层-隐藏层间权重的误差。\n",
    "\n",
    "要使用梯度下降法更新隐藏层的权重，你需要知道各隐藏层节点的误差对最终输出的影响。每层的输出是由两层间的权重决定的，两层之间产生的误差，按权重缩放后在网络中向前传播。既然我们知道输出误差，便可以用权重来反向传播到隐藏层。\n",
    "\n",
    "<img src=\"../../../sources/img/NeuralNetworkIntro/gradient-descent/backpropagation-text-0.png\">\n",
    "<img src=\"../../../sources/img/NeuralNetworkIntro/gradient-descent/backprop-network.png\">\n",
    "<img src=\"../../../sources/img/NeuralNetworkIntro/gradient-descent/backpropagation-text-1.png\">\n",
    "<img src=\"../../../sources/img/NeuralNetworkIntro/gradient-descent/backpropagation-text-2.png\">\n",
    "\n",
    "```\n",
    "hidden_error*inputs\n",
    "---------------------------------------------------------------------------\n",
    "ValueError                                Traceback (most recent call last)\n",
    "<ipython-input-22-3b59121cb809> in <module>()\n",
    "----> 1 hidden_error*x\n",
    "\n",
    "ValueError: operands could not be broadcast together with shapes (3,) (6,)\n",
    "```\n",
    "\n",
    "另外，现在 w_{ij}w 是一个矩阵，所以右侧对应也应该有跟左侧同样的维度。幸运的是，NumPy 这些都能搞定。如果你用一个列向量数组和一个行向量数组相乘，它会把列向量的第一个元素与行向量的每个元素相乘，组成一个新的二维数组的第一行。列向量的每一个元素依次重复该过程，最后你会得到一个二维数组，形状是 `(len(column_vector)`, `len(row_vector))`。\n",
    "\n",
    "```\n",
    "hidden_error*inputs[:,None]\n",
    "array([[ -8.24195994e-04,  -2.71771975e-04,   1.29713395e-03],\n",
    "       [ -2.87777394e-04,  -9.48922722e-05,   4.52909055e-04],\n",
    "       [  6.44605731e-04,   2.12553536e-04,  -1.01449168e-03],\n",
    "       [  0.00000000e+00,   0.00000000e+00,  -0.00000000e+00],\n",
    "       [  0.00000000e+00,   0.00000000e+00,  -0.00000000e+00],\n",
    "       [  0.00000000e+00,   0.00000000e+00,  -0.00000000e+00]])\n",
    "```\n",
    "\n",
    "这正好是我们计算权重更新的步长的方式。跟以前一样，如果你的输入是一个一行的二维数组，你可以用 `hidden_error*inputs.T`，但是如果 `inputs` 是一维数组，就不行了。\n",
    "\n",
    "## 反向传播练习\n",
    "\n",
    "接下来你将用代码来实现一次两个权重的反向传播更新。我们提供了正向传播的代码，你来实现反向传播的部分。\n",
    "要做的事\n",
    "\n",
    "- 计算网络输出误差\n",
    "- 计算输出层误差项\n",
    "- 用反向传播计算隐藏层误差项\n",
    "- 计算反向传播误差的权重更新步长"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 练习题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change in weights for hidden layer to output layer:\n",
      "[ 0.00804047  0.00555918]\n",
      "Change in weights for input layer to hidden layer:\n",
      "[[  1.77005547e-04  -5.11178506e-04]\n",
      " [  3.54011093e-05  -1.02235701e-04]\n",
      " [ -7.08022187e-05   2.04471402e-04]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.array([0.5, 0.1, -0.2])\n",
    "target = 0.6\n",
    "learnrate = 0.5\n",
    "\n",
    "weights_input_hidden = np.array([[0.5, -0.6],\n",
    "                                 [0.1, -0.2],\n",
    "                                 [0.1, 0.7]])\n",
    "\n",
    "weights_hidden_output = np.array([0.1, -0.3])\n",
    "\n",
    "## 正向传播\n",
    "hidden_layer_input = np.dot(x, weights_input_hidden)\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_output, weights_hidden_output)\n",
    "output = sigmoid(output_layer_in)\n",
    "\n",
    "## 反向传播\n",
    "## 计算网络输出误差\n",
    "error = target - output\n",
    "\n",
    "# 计算输出层误差项\n",
    "output_error_term = error * output * (1 - output) \n",
    "\n",
    "# 用反向传播计算隐藏层误差项 \n",
    "hidden_error_term = np.dot(output_error_term, weights_hidden_output) * \\\n",
    "                    hidden_layer_output * (1 - hidden_layer_output)\n",
    "\n",
    "# 计算隐藏层的权重更新到输出层\n",
    "delta_w_h_o = learnrate * output_error_term * hidden_layer_output\n",
    "\n",
    "# 计算输入层的权重更新到隐藏层 \n",
    "delta_w_i_h = learnrate * hidden_error_term * x[:, None]\n",
    "\n",
    "print('Change in weights for hidden layer to output layer:')\n",
    "print(delta_w_h_o)\n",
    "print('Change in weights for input layer to hidden layer:')\n",
    "print(delta_w_i_h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
