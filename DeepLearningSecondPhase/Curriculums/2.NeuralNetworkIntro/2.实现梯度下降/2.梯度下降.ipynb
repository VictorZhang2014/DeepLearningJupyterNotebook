{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学习权重\n",
    "\n",
    "你了解了如何使用感知器来构建 AND 和 XOR 运算，但它们的权重都是人为设定的。如果你要进行一个运算，例如预测大学录取结果，但你不知道正确的权重是什么，该怎么办？你要从样本中学习权重，然后用这些权重来做预测。\n",
    "\n",
    "要了解我们将如何找到这些权重，可以从我们的目标开始考虑。我们想让网络做出的预测与真实值尽可能接近。为了能够衡量，我们需要有一个指标来了解预测有多差，也就是误差 (error)。一个普遍的指标是误差平方和 sum of the squared errors (SSE)：\n",
    "\n",
    "<img src=\"../../../sources/img/NeuralNetworkIntro/gradient-descent/gradient_descent_0.png\">\n",
    "\n",
    "## 梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"800\" height=\"450\" src=\"https://www.youtube.com/embed/29PmNG7fuuM\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"800\" height=\"450\" src=\"https://www.youtube.com/embed/29PmNG7fuuM\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如 Luis 所说，用梯度下降，我们通过多个小步骤来实现目标。在这个例子中，我们希望一步一步改变权重来减小误差。借用前面的比喻，误差就像是山，我们希望走到山下。下山最快的路应该是最陡峭的那个方向，因此我们也应该寻找能够使误差最小化的方向。我们可以通过计算误差平方的梯度来找到这个方向。\n",
    "\n",
    "梯度是改变率或者斜度的另一个称呼。如果你需要回顾这个概念，可以看下可汗学院对这个问题的[讲解](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/gradient)。\n",
    "\n",
    "<img src=\"../../../sources/img/NeuralNetworkIntro/gradient-descent/gradient-descent-1.png\">\n",
    "\n",
    "<img src=\"../../../sources/img/NeuralNetworkIntro/gradient-descent/derivative-example.png\">\n",
    "\n",
    "梯度就是对多变量函数导数的泛化。我们可以用微积分来寻找误差函数中任意一点的梯度，它与输入权重有关，下一节你可以看到如何推导梯度下降的步骤。\n",
    "\n",
    "下面我画了一个拥有两个输入的神经网络误差示例，相应的，它有两个权重。你可以将其看成一个地形图，同一条线代表相同的误差，较深的线对应较大的误差。\n",
    "\n",
    "每一步，你计算误差和梯度，然后用它们来决定如何改变权重。重复这个过程直到你最终找到接近误差函数最小值的权重，即中间的黑点。\n",
    "\n",
    "<img src=\"../../../sources/img/NeuralNetworkIntro/gradient-descent/gradient-descent.png\">\n",
    "\n",
    "\n",
    "\n",
    "## 注意事项\n",
    "\n",
    "因为权重会走向梯度带它去的位置，它们有可能停留在误差小，但不是最小的地方。这个点被称作局部最低点。如果权重初始值有错，梯度下降可能会使得权重陷入局部最优，例如下图所示。\n",
    "\n",
    "<img src=\"../../../sources/img/NeuralNetworkIntro/gradient-descent/local-minima.png\">\n",
    "\n",
    "有方法可以避免这一点，被称作 [momentum](http://sebastianruder.com/optimizing-gradient-descent/index.html#momentum) .\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
