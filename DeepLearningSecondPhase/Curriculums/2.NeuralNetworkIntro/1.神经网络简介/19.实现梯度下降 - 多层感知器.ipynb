{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多层感知器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://www.youtube.com/embed/Rs9petvTBLk\" width=\"800\" height=\"450\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe src=\"https://www.youtube.com/embed/Rs9petvTBLk\" width=\"800\" height=\"450\" frameborder=\"0\" allowfullscreen></iframe>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.实现隐藏层\n",
    "\n",
    "先修要求\n",
    "接下来我们会讲神经网络在多层感知器里面的数学部分。讲多层感知器我们会用到向量和矩阵。你可以通过下列讲解对此做个回顾\n",
    "\n",
    "- 1.可汗学院的 [向量入门](https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/vectors/v/vector-introduction-linear-algebra).\n",
    "- 2.可汗学院的 [矩阵入门](https://www.khanacademy.org/math/precalculus/precalc-matrices).\n",
    "\n",
    "\n",
    "<img src=\"../../sources/img/NeuralNetworkIntro/gradient-descent-multilayer-perceptron-1.png\" alt=\"\" width=\"1200\" height=\"300\" />\n",
    "<img src=\"../../sources/img/NeuralNetworkIntro/gradient-descent-multilayer-perceptron-2.png\" alt=\"\" width=\"1200\" height=\"300\" />\n",
    "<img src=\"../../sources/img/NeuralNetworkIntro/gradient-descent-multilayer-perceptron-3.png\" alt=\"\" width=\"1200\" height=\"300\" />\n",
    "<img src=\"../../sources/img/NeuralNetworkIntro/gradient-descent-multilayer-perceptron-4.png\" alt=\"\" width=\"1200\" height=\"300\" />\n",
    "<img src=\"../../sources/img/NeuralNetworkIntro/gradient-descent-multilayer-perceptron-5.png\" alt=\"\" width=\"1200\" height=\"300\" />\n",
    "<img src=\"../../sources/img/NeuralNetworkIntro/gradient-descent-multilayer-perceptron-6.png\" alt=\"\" width=\"1200\" height=\"300\" />\n",
    "<img src=\"../../sources/img/NeuralNetworkIntro/gradient-descent-multilayer-perceptron-7.png\" alt=\"\" width=\"1200\" height=\"300\" />\n",
    "<img src=\"../../sources/img/NeuralNetworkIntro/gradient-descent-multilayer-perceptron-8.png\" alt=\"\" width=\"1200\" height=\"300\" />\n",
    "<img src=\"../../sources/img/NeuralNetworkIntro/gradient-descent-multilayer-perceptron-9.png\" alt=\"\" width=\"1200\" height=\"300\" />\n",
    "<img src=\"../../sources/img/NeuralNetworkIntro/gradient-descent-multilayer-perceptron-10.png\" alt=\"\" width=\"1200\" height=\"300\" />\n",
    "<img src=\"../../sources/img/NeuralNetworkIntro/gradient-descent-multilayer-perceptron-11.png\" alt=\"\" width=\"1200\" height=\"300\" />\n",
    "<img src=\"../../sources/img/NeuralNetworkIntro/gradient-descent-multilayer-perceptron-12.png\" alt=\"\" width=\"1200\" height=\"300\" />\n",
    "\n",
    "这里的规则是，如果是数组在左边，数组的元素个数必须与右边矩阵的行数一样。如果矩阵在左边，那么矩阵的列数，需要与右边向量的行数匹配。\n",
    "\n",
    "### 构建一个列向量\n",
    "\n",
    "看上面的介绍，你有时会需要一个列向量，尽管 NumPy 默认是行向量。你可以用 `arr.T` 来对数组进行转置，但对一维数组来说，转置还是行向量。所以你可以用 `arr[:,None]` 来创建一个列向量：\n",
    "\n",
    "```\n",
    "print(features)\n",
    "> array([ 0.49671415, -0.1382643 ,  0.64768854])\n",
    "\n",
    "print(features.T)\n",
    "> array([ 0.49671415, -0.1382643 ,  0.64768854])\n",
    "\n",
    "print(features[:, None])\n",
    "> array([[ 0.49671415],\n",
    "       [-0.1382643 ],\n",
    "       [ 0.64768854]])\n",
    "```\n",
    "\n",
    "当然，你可以创建一个二维数组，然后用 `arr.T` 得到列向量。\n",
    "\n",
    "```\n",
    "\n",
    "np.array(features, ndmin=2)\n",
    "> array([[ 0.49671415, -0.1382643 ,  0.64768854]])\n",
    "\n",
    "np.array(features, ndmin=2).T\n",
    "> array([[ 0.49671415],\n",
    "       [-0.1382643 ],\n",
    "       [ 0.64768854]])\n",
    "```\n",
    "\n",
    "我个人更倾向于保持所有向量为一维数组，这样可以更好认知。\n",
    "\n",
    "## 编程练习\n",
    "\n",
    "下面你要实现一个 4x3x2 网络的正向传播，用 sigmoid 作为两层的激活函数。\n",
    "\n",
    "要做的事情：\n",
    "\n",
    "- 计算隐藏层的输入\n",
    "- 计算隐藏层输出\n",
    "- 计算输出层的输入\n",
    "- 计算神经网络的输出\n",
    "\n",
    "\n",
    "## 练习题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Network size\n",
    "N_input = 4\n",
    "N_hidden = 3\n",
    "N_output = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "# Make some fake data\n",
    "X = np.random.randn(4)\n",
    "\n",
    "weights_input_to_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))\n",
    "weights_hidden_to_output = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))\n",
    "\n",
    "\n",
    "# TODO: Make a forward pass through the network\n",
    "\n",
    "hidden_layer_in = None\n",
    "hidden_layer_out = None\n",
    "\n",
    "print('Hidden-layer Output:')\n",
    "print(hidden_layer_out)\n",
    "\n",
    "output_layer_in = None\n",
    "output_layer_out = None\n",
    "\n",
    "print('Output-layer Output:')\n",
    "print(output_layer_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解决方案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Network size\n",
    "N_input = 4\n",
    "N_hidden = 3\n",
    "N_output = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "# Make some fake data\n",
    "X = np.random.randn(4)\n",
    "\n",
    "weights_input_to_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))\n",
    "weights_hidden_to_output = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))\n",
    "\n",
    "\n",
    "# TODO: Make a forward pass through the network\n",
    "\n",
    "hidden_layer_in = np.dot(X, weights_input_to_hidden)\n",
    "hidden_layer_out = sigmoid(hidden_layer_in)\n",
    "\n",
    "print('Hidden-layer Output:')\n",
    "print(hidden_layer_out)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_out, weights_hidden_to_output)\n",
    "output_layer_out = sigmoid(output_layer_in)\n",
    "\n",
    "print('Output-layer Output:')\n",
    "print(output_layer_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
