{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Batching\n",
    "\n",
    "在这一节，你将了解什么是 mini-batching，以及如何在 TensorFlow 里应用它。\n",
    "\n",
    "Mini-batching 是一个一次训练数据集的一小部分，而不是整个训练集的技术。它可以使内存较小、不能同时训练整个数据集的电脑也可以训练模型。\n",
    "\n",
    "Mini-batching 从运算角度来说是低效的，因为你不能在所有样本中计算 loss。但是这点小代价也比根本不能运行模型要划算。\n",
    "\n",
    "它跟随机梯度下降（SGD）结合在一起用也很有帮助。方法是在每一代训练之前，对数据进行随机混洗，然后创建 mini-batches，对每一个 mini-batch，用梯度下降训练网络权重。因为这些 batches 是随机的，你其实是在对每个 batch 做随机梯度下降（SGD）。\n",
    "\n",
    "让我们看看你的机器能否训练出 MNIST 数据集的权重和偏置项。\n",
    "\n",
    "```\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "```\n",
    "\n",
    "## 问题1\n",
    "\n",
    "计算 `train_features`, `train_labels`, `weights`, 和 `bias` 分别占用了多少字节（byte）的内存。可以忽略头部空间，只需要计算实际需要多少内存来存储数据。\n",
    "\n",
    "你也可以看 [这里](https://en.wikipedia.org/wiki/Single-precision_floating-point_format) 了解一个 float32 占用多少内存。\n",
    "\n",
    "- <i>train_features Shape: (55000, 784) Type: float32</i>\n",
    "\n",
    "- <i>train_labels Shape: (55000, 10) Type: float32</i>\n",
    "\n",
    "- <i>weights Shape: (784, 10) Type: float32</i>\n",
    "\n",
    "- <i>bias Shape: (10,) Type: float32</i>\n",
    "\n",
    "\n",
    "### `train_features` 占用多少字节内存？\n",
    "答：\n",
    "\n",
    "### `train_labels` 占用多少字节内存？\n",
    "答：\n",
    "\n",
    "### `weights` 占用多少字节内存？\n",
    "答：\n",
    "\n",
    "### `bias` 占用多少字节内存？\n",
    "答：\n",
    "\n",
    "\n",
    "输入、权重和偏置项总共的内存空间需求是 `174MB`，并不是太多。你可以在 `CPU` 和 `GPU` 上训练整个数据集。\n",
    "\n",
    "但将来你要用到的数据集可能是以 G 来衡量，甚至更多。你可以买更多的内存，但是会很贵。例如一个 12GB 显存容量的 Titan X GPU 会超过 1000 美金。所以，为了在你自己机器上运行大模型，你需要学会用 mini-batching。\n",
    "\n",
    "让我们看下如何在 `TensorFlow` 下实现 `mini-batching`\n",
    "\n",
    "\n",
    "\n",
    "## TensorFlow Mini-batching\n",
    "\n",
    "要使用 mini-batching，你首先要把你的数据集分成 batch。\n",
    "\n",
    "不幸的是，有时候不可能把数据完全分割成相同数量的 batch。例如有 1000 个数据点，你想每个 batch 有 128 个数据。但是 1000 无法被 128 整除。你得到的结果是其中 7 个 batch 有 128 个数据点，一个 batch 有 104 个数据点。(7*128 + 1*104 = 1000)\n",
    "\n",
    "batch 里面的数据点数量会不同的情况下，你需要利用 TensorFlow 的 tf.placeholder() 函数来接收这些不同的 batch。\n",
    "\n",
    "继续上述例子，如果每个样本有 `n_input = 784` 特征，`n_classes = 10` 个可能的标签，`features` 的维度应该是 `[None, n_input]`，`labels` 的维度是 `[None, n_classes]`。\n",
    "\n",
    "```\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "```\n",
    "\n",
    "`None` 在这里做什么用呢？\n",
    "\n",
    "`None` 维度在这里是一个 batch size 的占位符。在运行时，TensorFlow 会接收任何大于 0 的 batch size。\n",
    "\n",
    "回到之前的例子，这个设置可以让你把 `features` 和 `labels` 给到模型。无论 batch 中包含 128，还是 104 个数据点。\n",
    "\n",
    "\n",
    "\n",
    "## 问题二\n",
    "\n",
    "下列参数，会有多少 batch，最后一个 batch 有多少数据点？\n",
    "\n",
    "- <i>features is (50000, 400)</i>\n",
    "\n",
    "- <i>labels is (50000, 10)</i>\n",
    "\n",
    "- <i>batch_size is 128</i>\n",
    "    \n",
    "    \n",
    "### 总共有多少 batch？\n",
    "答：391 = (50000 / 128) + 1\n",
    "\n",
    "\n",
    "### 最后一个 batch 有多少数据点？\n",
    "答：80 = 50000 - 391 * 128\n",
    "\n",
    "现在你知道了基本概念，让我们学习如何来实现 `mini-batching`\n",
    "\n",
    "\n",
    "\n",
    "## 问题三\n",
    "\n",
    "对 `features` 和 `labels` 实现一个 `batches` 函数。这个函数返回每个有最大 `batch_size` 数据点的 `batch`。下面有例子来说明一个示例 `batches` 函数的输出是什么。\n",
    "\n",
    "```\n",
    "# 4 个特征\n",
    "example_features = [\n",
    "    ['F11','F12','F13','F14'],\n",
    "    ['F21','F22','F23','F24'],\n",
    "    ['F31','F32','F33','F34'],\n",
    "    ['F41','F42','F43','F44']]\n",
    "# 4 个 label\n",
    "example_labels = [\n",
    "    ['L11','L12'],\n",
    "    ['L21','L22'],\n",
    "    ['L31','L32'],\n",
    "    ['L41','L42']]\n",
    "\n",
    "example_batches = batches(3, example_features, example_labels)\n",
    "```\n",
    "\n",
    "`example_batches` 变量如下：\n",
    "\n",
    "```\n",
    "[\n",
    "    # 分 2 个 batch:\n",
    "    #   第一个 batch 的 size 是 3\n",
    "    #   第二个 batch 的 size 是 1\n",
    "    [\n",
    "        # size 为 3 的第一个 Batch\n",
    "        [\n",
    "            # 3 个特征样本\n",
    "            # 每个样本有四个特征\n",
    "            ['F11', 'F12', 'F13', 'F14'],\n",
    "            ['F21', 'F22', 'F23', 'F24'],\n",
    "            ['F31', 'F32', 'F33', 'F34']\n",
    "        ], [\n",
    "            # 3 个标签样本\n",
    "            # 每个标签有两个 label\n",
    "            ['L11', 'L12'],\n",
    "            ['L21', 'L22'],\n",
    "            ['L31', 'L32']\n",
    "        ]\n",
    "    ], [\n",
    "        # size 为 1 的第二个 Batch \n",
    "        # 因为 batch size 是 3。所以四个样品中只有一个在这里。\n",
    "        [\n",
    "            # 1 一个样本特征\n",
    "            ['F41', 'F42', 'F43', 'F44']\n",
    "        ], [\n",
    "            # 1 个 label\n",
    "            ['L41', 'L42']\n",
    "        ]\n",
    "    ]\n",
    "]\n",
    "```\n",
    "\n",
    "\n",
    "在下面实现 `batches` 函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    assert len(features) == len(labels)\n",
    "    # TODO: Implement batching\n",
    "    output_batches = []\n",
    "    last_rows_features = features[batch_size:]\n",
    "    last_rows_labels = labels[batch_size:]\n",
    "    \n",
    "    up_array = np.array([features, labels])\n",
    "    bottom_array = np.array([last_rows_features, last_rows_labels])\n",
    "    \n",
    "    output_batches.append(up_array)\n",
    "    output_batches.append(bottom_array)\n",
    "    \n",
    "    return output_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[['F11', 'F12', 'F13', 'F14'],\n",
      "   ['F21', 'F22', 'F23', 'F24'],\n",
      "   ['F31', 'F32', 'F33', 'F34']],\n",
      "  [['L11', 'L12'], ['L21', 'L22'], ['L31', 'L32']]],\n",
      " [[['F41', 'F42', 'F43', 'F44']], [['L41', 'L42']]]]\n",
      "Extracting ../../../sources/files/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ../../../sources/files/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../../sources/files/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../../sources/files/mnist/t10k-labels-idx1-ubyte.gz\n",
      "(55000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import numpy as np\n",
    "\n",
    "# All the MNIST data is hosted on http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "# 4 Samples of features\n",
    "example_features = [\n",
    "    ['F11','F12','F13','F14'],\n",
    "    ['F21','F22','F23','F24'],\n",
    "    ['F31','F32','F33','F34'],\n",
    "    ['F41','F42','F43','F44']]\n",
    "# 4 Samples of labels\n",
    "example_labels = [\n",
    "    ['L11','L12'],\n",
    "    ['L21','L22'],\n",
    "    ['L31','L32'],\n",
    "    ['L41','L42']]\n",
    "\n",
    "# PPrint prints data structures like 2d arrays, so they are easier to read\n",
    "pprint(batches(3, example_features, example_labels))\n",
    "\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('../../../sources/files/mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "print(train_features.shape)\n",
    "print(test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们用 `mini-batching` 来把 `MNIST` 特征和目标分批给到线性模型。\n",
    "\n",
    "设定 `batch size`，用 `batches` 函数来分配所有数据。建议的 `batch size` 是 `128`，你也可以根据自己内存大小来改变它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    assert len(features) == len(labels)\n",
    "    outout_batches = []\n",
    "    \n",
    "    sample_size = len(features)\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        outout_batches.append(batch)\n",
    "        \n",
    "    return outout_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../../sources/files/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ../../../sources/files/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../../sources/files/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../../sources/files/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Test Accuracy: 0.10119999945163727\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# All the MNIST data is hosted on http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('../../../sources/files/mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "# TODO: Set batch size\n",
    "batch_size = 128\n",
    "assert batch_size is not None, 'You must set the batch size'\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # TODO: Train optimizer on all batches\n",
    "    for batch_features, batch_labels in batches(batch_size, train_features, train_labels):\n",
    "        sess.run(optimizer, feed_dict={features: batch_features, labels: batch_labels})\n",
    "\n",
    "    # Calculate accuracy for test dataset\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: test_features, labels: test_labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然准确度不高，但是你或许知道训练集不止用来训练一次。你可以用数据集多次训练一个模型。下一章节我们会讨论 \"epochs\" 这个话题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
