{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Dropout\n",
    "\n",
    "<img src=\"../../../sources/img/NeuralNetworkIntro/tensorflow/dropout-node.jpeg\" />\n",
    "\n",
    "<center> 图 1：来自论文 \"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\" </center>\n",
    "<center> (https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf) </center>\n",
    "\n",
    "`Dropout` 是一个`降低过拟合`的`正则化技术`。它在网络中暂时的丢弃一些单元（[神经元](https://en.wikipedia.org/wiki/Artificial_neuron)），以及与它们的前后相连的所有节点。图 1 是 `dropout` 的工作示意图。\n",
    "\n",
    "`TensorFlow` 提供了一个 `tf.nn.dropout()` 函数，你可以用来实现 `dropout`。\n",
    "\n",
    "让我们来看一个 `tf.nn.dropout()` 的使用例子。\n",
    "\n",
    "\n",
    "```\n",
    "keep_prob = tf.placeholder(tf.float32) # probability to keep units\n",
    "\n",
    "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)\n",
    "\n",
    "logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n",
    "```\n",
    "\n",
    "上面的代码展示了如何在神经网络中应用 `dropout`。\n",
    "\n",
    "`tf.nn.dropout()` 函数有两个参数：\n",
    "\n",
    "`hidden_layer`：你要应用 `dropout` 的 `tensor`\n",
    "`keep_prob`：任何一个给定单元的留存率（没有被丢弃的单元）\n",
    "`keep_prob` 可以让你调整丢弃单元的数量。为了补偿被丢弃的单元，`tf.nn.dropout()` 把所有保留下来的单元`（没有被丢弃的单元）* 1/keep_prob`\n",
    "\n",
    "在训练时，一个好的 `keep_prob` 初始值是0.5。\n",
    "\n",
    "在测试时，把 `keep_prob` 值设为 `1.0` ，这样保留所有的单元，最大化模型的能力。\n",
    "\n",
    "\n",
    "## 练习1\n",
    "\n",
    "下面的代码，哪里出问题了？\n",
    "\n",
    "语法没问题，但是测试准确率很低。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32) # probability to keep units\n",
    "\n",
    "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)\n",
    "\n",
    "logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n",
    "\n",
    "...\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        for batch_i in range(batches):\n",
    "            ....\n",
    "\n",
    "            sess.run(optimizer, feed_dict={\n",
    "                features: batch_features,\n",
    "                labels: batch_labels,\n",
    "                keep_prob: 0.5})\n",
    "\n",
    "    validation_accuracy = sess.run(accuracy, feed_dict={\n",
    "        features: test_features,\n",
    "        labels: test_labels,\n",
    "        keep_prob: 0.5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 上面代码错在哪里？\n",
    "\n",
    "- `Dropout` 与 `Batching` 不能同时使用\n",
    "- `keep_prob` 设为 `0.5` 太低了\n",
    "- 在测试准确率的时候不应该传值给 `keep_prob`\n",
    "- `keep_prob` 在评估验证时应该设置为 `1.0`\n",
    "\n",
    "\n",
    "## 练习 2\n",
    "\n",
    "这个练习的代码来自 `ReLU` 的练习，应用一个 `dropout` 层。用 `ReLU` 层和 `dropout` 层构建一个模型，`keep_prob` 值设为 `0.5`。打印这个模型的 `logits`。\n",
    "\n",
    "注意: 由于 `dropout` 会随机丢弃单元，每次运行代码输出会有所不同。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  7.67999935  15.05999947]\n",
      " [  0.11200001   0.67200011]\n",
      " [ 33.74000168  43.38000107]]\n"
     ]
    }
   ],
   "source": [
    "# Solution is available in the other \"solution.py\" tab\n",
    "import tensorflow as tf\n",
    "\n",
    "hidden_layer_weights = [\n",
    "    [0.1, 0.2, 0.4],\n",
    "    [0.4, 0.6, 0.6],\n",
    "    [0.5, 0.9, 0.1],\n",
    "    [0.8, 0.2, 0.8]]\n",
    "out_weights = [\n",
    "    [0.1, 0.6],\n",
    "    [0.2, 0.1],\n",
    "    [0.7, 0.9]]\n",
    "\n",
    "# Weights and biases\n",
    "weights = [\n",
    "    tf.Variable(hidden_layer_weights),\n",
    "    tf.Variable(out_weights)]\n",
    "biases = [\n",
    "    tf.Variable(tf.zeros(3)),\n",
    "    tf.Variable(tf.zeros(2))]\n",
    "\n",
    "# Input\n",
    "features = tf.Variable([[0.0, 2.0, 3.0, 4.0], [0.1, 0.2, 0.3, 0.4], [11.0, 12.0, 13.0, 14.0]])\n",
    "\n",
    "# TODO: Create Model with Dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)\n",
    "\n",
    "logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n",
    "\n",
    "# TODO: Print logits from a session\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    print(session.run(logits, feed_dict={keep_prob: 0.5}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
