{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"950\" height=\"650\" src=\"https://www.youtube.com/embed/l9vg_1YUlzg\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"950\" height=\"650\" src=\"https://www.youtube.com/embed/l9vg_1YUlzg\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和神经网络一样，我们通过首先创建一个序列模型来创建一个 CNN。\n",
    "\n",
    "```\n",
    "from keras.models import Sequential\n",
    "```\n",
    "\n",
    "导入几个层，包括熟悉的神经网络层，以及在这节课学习的新的层。\n",
    "\n",
    "```\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "```\n",
    "\n",
    "和神经网络一样，通过使用 `.add()` 方法向网络中添加层级：\n",
    "\n",
    "```\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "```\n",
    "\n",
    "该网络以三个卷积层（后面跟着最大池化层）序列开始。前 6 个层级旨在将图片像素数组输入转换为所有空间休息都丢失、仅保留图片内容信息的数组 。然后在 `CNN` 的第七个层级将该数组扁平化为向量。后面跟着两个密集层，旨在进一步说明图片中的内容。最后一层针对数据集中的每个对象类别都有一个条目，并具有一个 `softmax` 激活函数，使其返回概率。\n",
    "\n",
    "** 注意 ** ：在该视频中，你可能注意到卷积层表示为 `Convolution2D`，而不是 `Conv2D`。对于 `Keras 2.0` 来说，二者都可以，但是最好使用 `Conv2D`。\n",
    "\n",
    "\n",
    "## 注意事项\n",
    "\n",
    "- 始终向 `CNN` 中的 `Conv2D` 层添加 `ReLU` 激活函数。但是网络的最后层级除外，密集层也应该具有 `ReLU` 激活函数。\n",
    "- 在构建分类网络时，网络中的最后层级应该是具有 `softmax` 激活函数的 密集层。最后层级的节点数量应该等于数据集中的类别总数。\n",
    "- 要开心！如果你觉得有点泄气，建议参阅 [Andrej Karpathy的tumblr](https://lossfunctions.tumblr.com/)，其中包含了用户提交的损失函数，对应的是本身有问题的模型。损失函数在训练期间应该是减小的，但是这些图表显示的却是非常不同的行为 :)。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
